{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc70168f",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "from swarmauri.llms.concrete.GroqModel import GroqModel as LLM\n",
    "from swarmauri.conversations.concrete.Conversation import Conversation\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "if API_KEY:\n",
    "    llm = GroqModel(api_key=API_KEY)\n",
    "    print(\"LLM initialized successfully.\")\n",
    "else:\n",
    "    print(\"Error: GROQ_API_KEY not found in environment variables. Please set it before running the script.\")   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "219dba53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents added to vector store successfully.\n",
      "All documents in vector store:\n",
      "This is a sample document.\n",
      "This is another sample document.\n",
      "This is a third sample document.\n"
     ]
    }
   ],
   "source": [
    "from swarmauri.vector_stores.concrete.TfidfVectorStore import TfidfVectorStore\n",
    "from swarmauri.documents.concrete.Document import Document\n",
    "\n",
    "vector_store = TfidfVectorStore()\n",
    "\n",
    "documents= [\n",
    "    Document(content=\"This is a sample document.\"),\n",
    "    Document(content=\"This is another sample document.\"),\n",
    "    Document(content=\"This is a third sample document.\")\n",
    "]\n",
    "vector_store.add_documents(documents)\n",
    "print(\"Documents added to vector store successfully.\")\n",
    "\n",
    "all_docs=vector_store.get_all_documents()\n",
    "print(\"All documents in vector store:\")\n",
    "for doc in all_docs:\n",
    "    print(doc.content)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad575c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "84d06146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current conversation History\n",
      "You are a helpful assistant.\n",
      "What is the capital of France?\n"
     ]
    }
   ],
   "source": [
    "from swarmauri.conversations.concrete.MaxSystemContextConversation import MaxSystemContextConversation\n",
    "from swarmauri.messages.concrete.SystemMessage import SystemMessage\n",
    "from swarmauri.messages.concrete.HumanMessage import HumanMessage\n",
    "\n",
    "system_context=SystemMessage(content=\"You are a helpful assistant.\")\n",
    "conversation = MaxSystemContextConversation(system_context=system_context)  \n",
    "user_message = HumanMessage(content=\"What is the capital of France?\")\n",
    "conversation.add_message(user_message)  \n",
    "\n",
    "print(\"current conversation History\")\n",
    "for message in conversation.history:\n",
    "    print(message.content)\n",
    "\n",
    "\n",
    "def get_allowed_models(llm):\n",
    "    failing_llms=[\n",
    "        \"gemma2-9b-it\",\n",
    "        \"llama-2-13b-chat\",\n",
    "        \"llama-2-70b-chat\",\n",
    "        \"llama-3-8b-instruct\",\n",
    "        \"llama-3-70b-instruct\",\n",
    "    ]\n",
    "    return [model for model in llm.allowed_models if model not in failing_llms]\n",
    "\n",
    "    llm=LLM(apiu_key=API_KEY)\n",
    "    print(f\"Resource:{llm.resource}\")\n",
    "    print(f\"Type:{llm.type}\")\n",
    "    print(f\"Default Name:{llm.name}\")\n",
    "          \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d3405f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allowed models: ['gemma2-9b-it', 'llama-3.1-70b-versatile', 'llama-3.1-8b-instant', 'llama-3.2-11b-text-preview', 'llama-3.2-1b-preview', 'llama-3.2-3b-preview', 'llama-3.2-90b-text-preview', 'llama-guard-3-8b', 'llama3-70b-8192', 'llama3-8b-8192', 'llama3-groq-70b-8192-tool-use-preview', 'llama3-groq-8b-8192-tool-use-preview', 'llava-v1.5-7b-4096-preview', 'mixtral-8x7b-32768']\n"
     ]
    }
   ],
   "source": [
    "def get_allowed_models(llm):\n",
    "    failing_llms=[\n",
    "        \"gemma-7b-it\",\n",
    "        \"llama-2-13b-chat\",\n",
    "        \"llama-2-70b-chat\",\n",
    "        \"llama-3-8b-instruct\",\n",
    "        \"llama-3-70b-instruct\",\n",
    "    ]\n",
    "    return [model for model in llm.allowed_models if model not in failing_llms]\n",
    "\n",
    "    llm=LLM(api_key=API_KEY)\n",
    "    print(f\"Resource:{llm.resource}\")\n",
    "    print(f\"Type:{llm.type}\")\n",
    "    print(f\"Default Name:{llm.name}\")\n",
    "    \n",
    "allowed_models= get_allowed_models(llm)\n",
    "print(\"Allowed models:\", allowed_models)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "29fd2e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_context = \"You are a helpful assistant.\"\n",
    "conversation = MaxSystemContextConversation(system_context=system_context)\n",
    "human_message = HumanMessage(content=\"What is the capital of France?\")\n",
    "conversation.add_message(human_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0835c5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized successfully.\n",
      "Prediction:gemma2-9b-it: The capital of France is **Paris**.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from swarmauri.llms.concrete.GroqModel import GroqModel as LLM\n",
    "from swarmauri.conversations.concrete.Conversation import Conversation\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "if API_KEY:\n",
    "    llm = GroqModel(api_key=API_KEY)\n",
    "    print(\"LLM initialized successfully.\")\n",
    "else:\n",
    "    print(\"Error: GROQ_API_KEY not found in environment variables. Please set it before running the script.\") \n",
    "# Use the first allowed model that is not decommissioned\n",
    "for model in allowed_models:\n",
    "    if model != \"llama-3.1-8b-instant\":\n",
    "        llm.name = model\n",
    "        break\n",
    "\n",
    "conversation = Conversation()\n",
    "input_data = \"What is the capital of France?\"\n",
    "human_message = HumanMessage(content=input_data)\n",
    "conversation.add_message(human_message)\n",
    "llm.predict(conversation=conversation)\n",
    "prediction = conversation.get_last().content\n",
    "print(f\"Prediction:{llm.name}: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "608eef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from swarmauri.agents.concrete.RagAgent import RagAgent\n",
    "rag_system_context = \"You are a helpful assistant with access to a vector store.\"\n",
    "rag_conversation = MaxSystemContextConversation(system_context=rag_system_context)\n",
    "\n",
    "rag_agent = RagAgent(llm=llm, vector_store=vector_store, conversation=rag_conversation, system_context=rag_system_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b6df7b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: what cloud service do they use?\n",
      "RAG Response: I do not have access to real-time information, including details about specific cloud services used by companies or individuals. My knowledge is based on the training data I was provided, and I do not have insights into private information like that. \n",
      "\n",
      "\n",
      "To find out what cloud service a particular company or document might use, you would need to consult their public information, such as:\n",
      "\n",
      "* **Company website:** Many companies list their technology partners or infrastructure providers on their website.\n",
      "* **Press releases and blog posts:** Announcements about new deployments or partnerships often reveal the cloud service used.\n",
      "* **Technical documentation:** If the company publishes technical documentation for their products or services, it may mention the underlying cloud platform.\n",
      "* **Industry reports and analyses:** Research firms often publish reports on cloud adoption trends and market share, which may include information about specific companies. \n",
      "\n",
      "\n",
      "\n",
      "Query: what database do they use?\n",
      "RAG Response: Good luck with your research! \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries=[\n",
    "    \"what cloud service do they use?\",\n",
    "    \"what database do they use?\",\n",
    "]\n",
    "for query in queries:\n",
    "    response= rag_agent.exec(query)\n",
    "    print(f\"Query: {query}\\nRAG Response: {response}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
